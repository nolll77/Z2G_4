{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Jovian-Z2G-multilabel-stratification-cv-and-ensemble-Version_Final.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuwafS9i-M37",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/c/jovian-pytorch-z2g/discussion/163666#913477\n",
        "\n",
        "https://jovian.ml/mpsampat/multilabel-stratification-cv-and-ensemble\n",
        "\n",
        "https://jovian.ml/mpsampat/multilabel-stratification-cv-and-ensemble/compare\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bWmPy3s9LnP",
        "colab_type": "text"
      },
      "source": [
        "In addition to having multiple labels in each image, the other challenge in this competition is the existence of rare classes and combinations of different classes.\n",
        "\n",
        "One technique to deal with this is to guarantee a balanced spliting between training and validation set. The usual random `train_test_split` is not ideal in this case because you can end up putting rare cases in the validation set and your model will never learn about them. The stratification present in the `scikit-learn` is also not equipped to deal with multilabel targets. The library `scikit-multilearn` does exactly that.\n",
        "\n",
        "Update 1: in the previous example I've just showed how to create the splitted dataframe. This is not much help if you are not used to create datasets in Pytorch. In this version I show how to use this in conjunction with the Advanced Transfer Learning Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "o_hMjVNQ9LnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import multiprocessing as mp\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import make_grid\n",
        "from skmultilearn.model_selection import IterativeStratification\n",
        "%matplotlib inline\n",
        "\n",
        "      \n",
        "ROOT = Path('../')\n",
        "DIR = ROOT /'data/'\n",
        "TRAIN = DIR / 'train'\n",
        "TEST = DIR / 'test'\n",
        "arch = 'resnet18'\n",
        "freeze_epochs = 20\n",
        "unfreeze_epochs = 20\n",
        "epochs = freeze_epochs + unfreeze_epochs;\n",
        "size = 512\n",
        "if size == 256: batch_size = 128\n",
        "if size == 512: batch_size = 64\n",
        "    \n",
        "nfolds = 10\n",
        "threshold = 0.3\n",
        "SEED = 2020\n",
        "max_lr = 0.001\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "opt_func = torch.optim.Adam\n",
        "global advanced_fc;\n",
        "advanced_fc = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWscz9Z_9LnX",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO4ZZxvE9LnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_sample(img, target, invert=True):\n",
        "    if invert:\n",
        "        plt.imshow(1 - img.permute((1, 2, 0)))\n",
        "    else:\n",
        "        plt.imshow(img.permute(1, 2, 0))\n",
        "    print('Labels:', decode_target(target, text_labels=True))\n",
        "    \n",
        "def show_batch(dl, invert=True):\n",
        "    for images, labels in dl:\n",
        "        fig, ax = plt.subplots(figsize=(16, 8))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        data = 1-images if invert else images\n",
        "        ax.imshow(make_grid(data, nrow=16).permute(1, 2, 0))\n",
        "        break\n",
        "\n",
        "def F_score(output, label, threshold=threshold, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "    return F2.mean(0)\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "    \n",
        "class MultilabelImageClassificationBase(nn.Module):\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, targets = batch \n",
        "        out = self(images)   \n",
        "        #out = out.type(torch.FloatTensor).cuda() \n",
        "        #targets=targets.cuda()\n",
        "        #loss = F.binary_cross_entropy(out.type(torch.FloatTensor), targets)\n",
        "        loss = F.binary_cross_entropy(out, targets)      \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, targets = batch \n",
        "        out = self(images)                           # Generate predictions\n",
        "        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n",
        "        score = F_score(out, targets)\n",
        "        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_scores = [x['val_score'] for x in outputs]\n",
        "        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_score']))\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def encode_label(label):\n",
        "    target = torch.zeros(10)\n",
        "    for l in str(label).split(' '):\n",
        "        target[int(l)] = 1.\n",
        "    return target\n",
        "\n",
        "def decode_target(target, text_labels=False, threshold=threshold):\n",
        "    result = []\n",
        "    for i, x in enumerate(target):\n",
        "        if (x >= threshold):\n",
        "            if text_labels:\n",
        "                result.append(labels[i] + \"(\" + str(i) + \")\")\n",
        "            else:\n",
        "                result.append(str(i))\n",
        "    return ' '.join(result)\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBoC6W-o9Lne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(DIR / 'train.csv').set_index(\"Image\").sort_index()\n",
        "submission = pd.read_csv(ROOT / 'submission.csv') # Don't change the order in the submission\n",
        "DEVICE = get_default_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gsh5ZRni9Lnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = {int(x.stem): x for x in TRAIN.iterdir() if x.suffix == '.png'}\n",
        "test_images = {int(x.stem): x for x in TEST.iterdir() if x.suffix == '.png'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "1mvlFY-p9Lno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEToDq249Lnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_label(label):\n",
        "    target = torch.zeros(10)\n",
        "    for l in str(label).split(' '):\n",
        "        target[int(l)] = 1.\n",
        "    return target\n",
        "\n",
        "def decode_target(target, text_labels=False, threshold=threshold):\n",
        "    result = []\n",
        "    for i, x in enumerate(target):\n",
        "        if (x >= threshold):\n",
        "            if text_labels:\n",
        "                result.append(labels[i] + \"(\" + str(i) + \")\")\n",
        "            else:\n",
        "                result.append(str(i))\n",
        "    return ' '.join(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUy16uQq9Ln0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = {\n",
        "    0: 'Mitochondria',\n",
        "    1: 'Nuclear bodies',\n",
        "    2: 'Nucleoli',\n",
        "    3: 'Golgi apparatus',\n",
        "    4: 'Nucleoplasm',\n",
        "    5: 'Nucleoli fibrillar center',\n",
        "    6: 'Cytosol',\n",
        "    7: 'Plasma membrane',\n",
        "    8: 'Centrosome',\n",
        "    9: 'Nuclear speckles'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IAnQEfD9Ln_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexes = {v:k for k,v in labels.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZWtrUxl9LoE",
        "colab_type": "text"
      },
      "source": [
        "## Count the distribution of combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un-rOiBa9LoG",
        "colab_type": "code",
        "colab": {},
        "outputId": "49aec939-c4d8-4ecb-c43d-07d0e08fc11e"
      },
      "source": [
        "df.Label.value_counts().tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6 1 4 7    2\n",
              "6 1 5 4    2\n",
              "6 9 5      2\n",
              "8 6 9      1\n",
              "8 9 7      1\n",
              "8 6 0      1\n",
              "8 3 5 4    1\n",
              "8 6 5      1\n",
              "0 5 4 7    1\n",
              "6 0 2 4    1\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6WnpA549LoN",
        "colab_type": "text"
      },
      "source": [
        "We can see we have combinations with only one example, this is certainly harder for our model to generalize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ8Jm6k39LoO",
        "colab_type": "text"
      },
      "source": [
        "## Split the label strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNXbp3d39LoP",
        "colab_type": "code",
        "colab": {},
        "outputId": "0af0511e-cedc-4b03-ee83-ae611e61e0f6"
      },
      "source": [
        "df['Label'] = df.Label.str.split(\" \") ; df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[9, 4, 7]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[3, 2, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[5]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[3, 4]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[4]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Label\n",
              "Image           \n",
              "0      [9, 4, 7]\n",
              "1      [3, 2, 4]\n",
              "3            [5]\n",
              "4         [3, 4]\n",
              "6            [4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIoZnmdQ9LoT",
        "colab_type": "text"
      },
      "source": [
        "## Expand the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i18Jr7YZ9LoU",
        "colab_type": "code",
        "colab": {},
        "outputId": "3f01e7b9-4244-44dc-e34c-664787e604cc"
      },
      "source": [
        "df = df.explode('Label') ; df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label\n",
              "Image      \n",
              "0         9\n",
              "0         4\n",
              "0         7\n",
              "1         3\n",
              "1         2\n",
              "1         4\n",
              "3         5\n",
              "4         3\n",
              "4         4\n",
              "6         4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qZv4VBp9LoZ",
        "colab_type": "text"
      },
      "source": [
        "## Count the distributions of individual classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGsSyYJK9Loa",
        "colab_type": "code",
        "colab": {},
        "outputId": "7251ce24-a050-44fc-f53c-7cc044f43f12"
      },
      "source": [
        "df.Label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    9066\n",
              "6    5711\n",
              "7    2629\n",
              "2    2542\n",
              "0    2088\n",
              "3    1977\n",
              "1    1752\n",
              "9    1278\n",
              "5    1109\n",
              "8    1037\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm0lfefO9Lof",
        "colab_type": "text"
      },
      "source": [
        "## Turn labels into one-hot encoding columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK9HBuWe9Log",
        "colab_type": "code",
        "colab": {},
        "outputId": "8a3d2229-c426-492f-b9bd-037bcb48ff00"
      },
      "source": [
        "df = pd.get_dummies(df) ; df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label_0</th>\n",
              "      <th>Label_1</th>\n",
              "      <th>Label_2</th>\n",
              "      <th>Label_3</th>\n",
              "      <th>Label_4</th>\n",
              "      <th>Label_5</th>\n",
              "      <th>Label_6</th>\n",
              "      <th>Label_7</th>\n",
              "      <th>Label_8</th>\n",
              "      <th>Label_9</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Label_0  Label_1  Label_2  Label_3  Label_4  Label_5  Label_6  Label_7  \\\n",
              "Image                                                                           \n",
              "0            0        0        0        0        0        0        0        0   \n",
              "0            0        0        0        0        1        0        0        0   \n",
              "0            0        0        0        0        0        0        0        1   \n",
              "1            0        0        0        1        0        0        0        0   \n",
              "1            0        0        1        0        0        0        0        0   \n",
              "\n",
              "       Label_8  Label_9  \n",
              "Image                    \n",
              "0            0        1  \n",
              "0            0        0  \n",
              "0            0        0  \n",
              "1            0        0  \n",
              "1            0        0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwU38oD99Lok",
        "colab_type": "text"
      },
      "source": [
        "## Combine the vectors of the same index and sum them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yYRRNOE9Lol",
        "colab_type": "code",
        "colab": {},
        "outputId": "440ba99c-ad2c-4c89-d314-483094b1a2f5"
      },
      "source": [
        "df = df.groupby(df.index).sum() ; df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label_0</th>\n",
              "      <th>Label_1</th>\n",
              "      <th>Label_2</th>\n",
              "      <th>Label_3</th>\n",
              "      <th>Label_4</th>\n",
              "      <th>Label_5</th>\n",
              "      <th>Label_6</th>\n",
              "      <th>Label_7</th>\n",
              "      <th>Label_8</th>\n",
              "      <th>Label_9</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Label_0  Label_1  Label_2  Label_3  Label_4  Label_5  Label_6  Label_7  \\\n",
              "Image                                                                           \n",
              "0            0        0        0        0        1        0        0        1   \n",
              "1            0        0        1        1        1        0        0        0   \n",
              "3            0        0        0        0        0        1        0        0   \n",
              "4            0        0        0        1        1        0        0        0   \n",
              "6            0        0        0        0        1        0        0        0   \n",
              "\n",
              "       Label_8  Label_9  \n",
              "Image                    \n",
              "0            0        1  \n",
              "1            0        0  \n",
              "3            0        0  \n",
              "4            0        0  \n",
              "6            0        0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROtoQT1Q9Loq",
        "colab_type": "code",
        "colab": {},
        "outputId": "7ed50927-4faf-4c90-9c0a-86b7bec88b9e"
      },
      "source": [
        "df.columns = labels.keys() ; df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1  2  3  4  5  6  7  8  9\n",
              "Image                              \n",
              "0      0  0  0  0  1  0  0  1  0  1\n",
              "1      0  0  1  1  1  0  0  0  0  0\n",
              "3      0  0  0  0  0  1  0  0  0  0\n",
              "4      0  0  0  1  1  0  0  0  0  0\n",
              "6      0  0  0  0  1  0  0  0  0  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YpG5sCi9Loz",
        "colab_type": "text"
      },
      "source": [
        "Turn the index (image names) and columns (one-hot target) into np.arrays to feed the stratification algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHQsJr_09Lo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = df.index.values, df.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g69aulJi9Lo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_fold = IterativeStratification(n_splits=nfolds, order=2)\n",
        "\n",
        "splits = list(k_fold.split(X, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNnTGlQG9Lo9",
        "colab_type": "text"
      },
      "source": [
        "In the previous example I've used the `order=1` option. Reading the documentation it says is better advised to use higher orders for the model to sample with replacement for more rare classes. Experiment with these values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fARqjgxB9Lo-",
        "colab_type": "code",
        "colab": {},
        "outputId": "b8728764-7e3a-4bfc-a7dd-f5977e2ecf18"
      },
      "source": [
        "splits[0][0].shape , splits[0][1].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((17303,), (1933,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HJxq0Ha9LpC",
        "colab_type": "text"
      },
      "source": [
        "Now we have a list with 5 arrays to index and split our dataset. Each array has 2 dimensions, the 1st dimension are the indices of our training set ( 80% of the data ) and the second dimension are the indices of our validation set (20% of the data). A better way to index our data frame is to create a new column in our DataFrame with the split for that fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YggJN6-49LpD",
        "colab_type": "code",
        "colab": {},
        "outputId": "6c5551eb-a809-4163-a4b7-cd3d88c56b95"
      },
      "source": [
        "fold_splits = np.zeros(df.shape[0]).astype(np.int)\n",
        "\n",
        "for i in range(nfolds):\n",
        "    fold_splits[splits[i][1]] = i\n",
        "\n",
        "df['Split'] = fold_splits\n",
        "\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>Split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1  2  3  4  5  6  7  8  9  Split\n",
              "Image                                     \n",
              "0      0  0  0  0  1  0  0  1  0  1      3\n",
              "1      0  0  1  1  1  0  0  0  0  0      1\n",
              "3      0  0  0  0  0  1  0  0  0  0      2\n",
              "4      0  0  0  1  1  0  0  0  0  0      0\n",
              "6      0  0  0  0  1  0  0  0  0  0      4\n",
              "7      0  0  0  0  1  0  0  0  0  0      6\n",
              "8      1  1  0  0  0  0  0  0  0  0      3\n",
              "11     0  0  0  0  1  0  1  0  0  0      9\n",
              "12     0  0  0  0  1  0  1  0  0  0      4\n",
              "13     0  0  0  0  0  0  0  0  0  1      8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwoI8sJV9LpH",
        "colab_type": "text"
      },
      "source": [
        "So, for example for our `fold=0`, all the examples with `Split == 0` is our validation set, all the other are our training set for that fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP1l6F6-9LpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold = 0\n",
        "\n",
        "train_df = df[df.Split != fold]\n",
        "val_df = df[df.Split == fold]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpj1p97C9LpL",
        "colab_type": "code",
        "colab": {},
        "outputId": "13a83e70-6548-4064-d164-042c44732a04"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>Split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1  2  3  4  5  6  7  8  9  Split\n",
              "Image                                     \n",
              "0      0  0  0  0  1  0  0  1  0  1      3\n",
              "1      0  0  1  1  1  0  0  0  0  0      1\n",
              "3      0  0  0  0  0  1  0  0  0  0      2\n",
              "6      0  0  0  0  1  0  0  0  0  0      4\n",
              "7      0  0  0  0  1  0  0  0  0  0      6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1knF78t9LpR",
        "colab_type": "code",
        "colab": {},
        "outputId": "403ad99f-2825-4a78-fec5-8e22db0f7857"
      },
      "source": [
        "val_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>Split</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Image</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0  1  2  3  4  5  6  7  8  9  Split\n",
              "Image                                     \n",
              "4      0  0  0  1  1  0  0  0  0  0      0\n",
              "19     0  1  0  0  0  0  0  0  0  0      0\n",
              "20     0  0  1  0  0  0  0  1  0  0      0\n",
              "37     0  0  0  0  0  1  0  0  0  0      0\n",
              "42     0  0  0  0  1  0  1  0  0  0      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwvO6K4T9LpV",
        "colab_type": "text"
      },
      "source": [
        "The following decoded dataframes are for visualization purposes only. We will pass the above dataframes with one-hot encoded labels to our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRGl7KZZ9LpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_train_df = pd.DataFrame({'Label' : list(map(decode_target, train_df.values))}, index=train_df.index)\n",
        "decoded_val_df = pd.DataFrame({'Label' : list(map(decode_target, val_df.values))}, index=val_df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJzpMpsk9Lpa",
        "colab_type": "code",
        "colab": {},
        "outputId": "ad262822-86c3-45d2-91d4-86255b0da4bf"
      },
      "source": [
        "decoded_train_df.Label.value_counts().tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0 3 10        1\n",
              "5 6 8 10      1\n",
              "1 4 6 7 10    1\n",
              "0 6 8 10      1\n",
              "3 4 5 8 10    1\n",
              "0 4 5 7 10    1\n",
              "0 7 8 10      1\n",
              "7 8 9 10      1\n",
              "0 2 4 6 10    1\n",
              "6 8 9 10      1\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whP_dQ8X9Lpf",
        "colab_type": "code",
        "colab": {},
        "outputId": "ff6b1471-2f84-4f7f-9437-6d773f54ff99"
      },
      "source": [
        "decoded_val_df.Label.value_counts().tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0 2 7      1\n",
              "1 4 6 7    1\n",
              "0 6 7      1\n",
              "6 7 9      1\n",
              "8 9        1\n",
              "3 5        1\n",
              "0 3        1\n",
              "0 4 5      1\n",
              "0 7 8      1\n",
              "0 7        1\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96P35pys9Lpj",
        "colab_type": "text"
      },
      "source": [
        "## How to use this for cross-validation and training\n",
        "The following function organizes the code above return a list with `nfolds` where each item is a tuple with the `train_df` and `val_df` for that fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGyMI9On9Lpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_split_df(nfolds, order=1):\n",
        "\n",
        "    df = pd.read_csv(DIR / 'train.csv').set_index(\"Image\")\n",
        "\n",
        "    submission = pd.read_csv(ROOT / 'submission.csv')\n",
        "\n",
        "    split_df = pd.get_dummies(df.Label.str.split(\" \").explode())\n",
        "\n",
        "    split_df = split_df.groupby(split_df.index).sum() \n",
        "\n",
        "    X, y = split_df.index.values, split_df.values\n",
        "\n",
        "    k_fold = IterativeStratification(n_splits=nfolds, order=order)\n",
        "\n",
        "    splits = list(k_fold.split(X, y))\n",
        "\n",
        "    fold_splits = np.zeros(df.shape[0]).astype(np.int)\n",
        "\n",
        "    for i in range(nfolds):\n",
        "        fold_splits[splits[i][1]] = i\n",
        "\n",
        "    split_df['Split'] = fold_splits    \n",
        "\n",
        "    df_folds = []\n",
        "\n",
        "    for fold in range(nfolds):\n",
        "\n",
        "        df_fold = split_df.copy()\n",
        "            \n",
        "        train_df = df_fold[df_fold.Split != fold].drop('Split', axis=1).reset_index()\n",
        "        \n",
        "        val_df = df_fold[df_fold.Split == fold].drop('Split', axis=1).reset_index()\n",
        "        \n",
        "        df_folds.append((train_df, val_df))\n",
        "\n",
        "    return df_folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9MZ0fHc9Lpo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = create_split_df(nfolds, order=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVkWVXd49Lps",
        "colab_type": "text"
      },
      "source": [
        "## Statistics of the DataSet\n",
        "To normalize your Dataset you can use the Imagenet Statistics or another way is to calculate the stats of your current images, train + test and instead use those. The following snippet of code does these. \n",
        "Uncomment if you want to try it yourself but I already provided the values. You can see the values are very different from imagenet. Experiment with both!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HWA6drU9Lps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train_set = set(TRAIN.iterdir())\n",
        "#test_set = set(TEST.iterdir())\n",
        "#whole_set = train_set.union(test_set)\n",
        "\n",
        "#x_tot, x2_tot = [], []\n",
        "#for file in tqdm(whole_set):\n",
        "#    img = cv2.imread(str(file), cv2.COLOR_RGB2BGR)\n",
        "#    img = img/255.0\n",
        "#    x_tot.append(img.reshape(-1, 3).mean(0))\n",
        "#    x2_tot.append((img**2).reshape(-1, 3).mean(0))\n",
        "\n",
        "#image stats\n",
        "#img_avr =  np.array(x_tot).mean(0)\n",
        "#img_std =  np.sqrt(np.array(x2_tot).mean(0) - img_avr**2)\n",
        "#print('mean:',img_avr, ', std:', np.sqrt(img_std))\n",
        "#mean = torch.as_tensor(x_tot)\n",
        "#std =torch.as_tensor(x2_tot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJDQCtM39Lpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = torch.tensor([0.05438065, 0.05291743, 0.07920227])\n",
        "std = torch.tensor([0.39414383, 0.33547948, 0.38544176])\n",
        "imagenet_mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "imagenet_std = torch.tensor([0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjr7PBbx9Lp2",
        "colab_type": "text"
      },
      "source": [
        "The following is and adaptation from [Advanced Transfer Learning Starter Notebook](https://www.kaggle.com/aakashns/advanced-transfer-learning-starter-notebook) using the Stratified Splits, Cross Validation and saving the best model weights per fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlwO2pLa9Lp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_tfms = T.Compose([\n",
        "  # T.RandomCrop(512, padding=8, padding_mode='reflect'),\n",
        "#     T.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n",
        "#    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    T.RandomHorizontalFlip(), \n",
        "    #T.RandomRotation(10),\n",
        "    T.RandomAffine(degrees=(-90,90),\n",
        "                   scale=(0.9,1.1),\n",
        "                  shear=(-10,10)),\n",
        "    T.ToTensor(), \n",
        "    T.Normalize(imagenet_mean, imagenet_std,inplace=True)\n",
        "    #T.Normalize(mean,std,inplace=True), \n",
        "    #T.RandomErasing(inplace=True)\n",
        "])\n",
        "\n",
        "valid_tfms = T.Compose([\n",
        "    #T.Resize(size), \n",
        "    T.ToTensor(), \n",
        "    T.Normalize(imagenet_mean, imagenet_std)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flqdZP5V9Lp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HumanProteinDataset(Dataset):\n",
        "    def __init__(self, df, transform=None, is_test=False):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.files = test_images if is_test else train_images\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.loc[idx]\n",
        "        img_id, img_label = int(row['Image']), row.drop('Image').values.astype(np.float32)\n",
        "        #img_label = np.float32(img_label)\n",
        "        img = self.files[img_id] \n",
        "        img = Image.open(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, img_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6a5WwHS9Lp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptiveConcatPool2d(nn.Module):\n",
        "    def __init__(self, sz=1):\n",
        "        super().__init__()\n",
        "        self.adavgp = nn.AdaptiveAvgPool2d(sz)\n",
        "        self.adamaxp = nn.AdaptiveMaxPool2d(sz)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.cat([self.adavgp(x), self.adamaxp(x)], 1)\n",
        "        x = x.view(x.size(0),-1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbWsQgCc9LqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self, in_features, intermed_bn= 512, out_features=10, dout=0.25):\n",
        "        super().__init__()\n",
        "        self.fc_bn0 = nn.BatchNorm1d(in_features)\n",
        "        self.dropout0 = nn.Dropout(dout)\n",
        "        self.fc0 = nn.Linear(in_features, intermed_bn, bias=True)\n",
        "        self.fc_bn1 = nn.BatchNorm1d(intermed_bn, momentum=0.01)\n",
        "        self.dropout1 = nn.Dropout(dout * 2)\n",
        "        self.fc1 = nn.Linear(intermed_bn, out_features, bias=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc_bn0(x)\n",
        "        x = self.dropout0(x)\n",
        "        x = F.relu(self.fc0(x))\n",
        "        x = self.fc_bn1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AERzZrB89LqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Proteinmodel(MultilabelImageClassificationBase):\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        # Use a pretrained model\n",
        "        self.network = encoder(pretrained=True)\n",
        "        # Replace last layer\n",
        "        \n",
        "        num_ftrs = self.network.fc.in_features\n",
        "        if advanced_fc == True:\n",
        "            self.network.avgpool = AdaptiveConcatPool2d()\n",
        "            self.network.fc = CustomClassifier(in_features=num_ftrs*2, out_features=10)\n",
        "        else:\n",
        "            self.network.fc = nn.Linear(num_ftrs, 10) \n",
        "    \n",
        "    def forward(self, xb):\n",
        "        return torch.sigmoid(self.network(xb))\n",
        "    \n",
        "    def freeze(self):\n",
        "        # To freeze the residual layers\n",
        "        for param in self.network.parameters():\n",
        "            param.require_grad = False\n",
        "        for param in self.network.fc.parameters():\n",
        "            param.require_grad = True\n",
        "    \n",
        "    def unfreeze(self):\n",
        "        # Unfreeze all layers\n",
        "        for param in self.network.parameters():\n",
        "            param.require_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSiDfgVR9LqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_split_dataloaders(split):\n",
        "    train_df, val_df = split\n",
        "    \n",
        "    train_ds = HumanProteinDataset(train_df, transform=train_tfms)\n",
        "    val_ds = HumanProteinDataset(val_df, transform=valid_tfms)\n",
        "    \n",
        "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=mp.cpu_count(), pin_memory=True)\n",
        "    val_dl = DataLoader(val_ds, batch_size, num_workers=mp.cpu_count(), pin_memory=True)\n",
        "    \n",
        "    \n",
        "    train_dl = DeviceDataLoader(train_dl, DEVICE)\n",
        "    val_dl = DeviceDataLoader(val_dl, DEVICE)\n",
        "    \n",
        "    return train_dl, val_dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my-RCZ1e9LqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_dl():\n",
        "    test_ds = HumanProteinDataset(submission, transform=valid_tfms, is_test=True)\n",
        "    test_dl = DataLoader(test_ds, batch_size, num_workers=mp.cpu_count(), pin_memory=True)\n",
        "    return DeviceDataLoader(test_dl, DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2G0VS0R9LqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    #outputs = [model.validation_step(batch) for batch in tqdm(val_loader)]\n",
        "    outputs = [model.validation_step(batch) for batch in (val_loader)]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD, save_best='val_loss'):\n",
        "    \n",
        "    since = time.time()\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up cutom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss, best_score = 1e4, 0.0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        #for batch in tqdm(train_loader):\n",
        "        for batch in (train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        \n",
        "        if result['val_loss'] < best_loss:   \n",
        "            best_loss = result['val_loss']\n",
        "            if save_best == 'val_loss':\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "            \n",
        "        if result['val_score'] > best_score:\n",
        "            best_score = result['val_score']                   \n",
        "            if save_best == 'val_score':            \n",
        "                best_model_wts = copy.deepcopy(model.state_dict())          \n",
        "        \n",
        "        history.append(result)\n",
        "        \n",
        "    time_elapsed = time.time() - since\n",
        "    \n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    \n",
        "    print(f'Best val Score: {best_score:4f}')\n",
        "    \n",
        "    print(f'Best val loss: {best_loss:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "        \n",
        "    \n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O33ICmDG9LqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_single(image):\n",
        "    xb = image.unsqueeze(0)\n",
        "    xb = to_device(xb, device)\n",
        "    preds = model(xb)\n",
        "    prediction = preds[0]\n",
        "    print(\"Prediction: \", prediction)\n",
        "    show_sample(image, prediction)\n",
        "    \n",
        "@torch.no_grad()\n",
        "def predict_dl(dl, model):\n",
        "    torch.cuda.empty_cache()\n",
        "    batch_probs = []\n",
        "    for xb, _ in (dl):\n",
        "    #for xb, _ in tqdm(dl):\n",
        "        probs = model(xb)\n",
        "        batch_probs.append(probs.cpu().detach())\n",
        "    batch_probs = torch.cat(batch_probs)\n",
        "    return batch_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO-RK5769Lqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = get_default_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wylbc7LG9Lqj",
        "colab_type": "code",
        "colab": {},
        "outputId": "49e85886-634a-4563-c74a-25224dc385de"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "histories = []\n",
        "predictions = []\n",
        "\n",
        "test_dl = get_test_dl()\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "\n",
        "for i, split in enumerate(splits):\n",
        "    \n",
        "    history = []\n",
        "    print(f'Starting training of train set for fold=={i}')\n",
        "    train_dl, val_dl = get_split_dataloaders(split)\n",
        "    \n",
        "    # initialize parameters of model to train each fold from scratch and not leak info from different folds\n",
        "    if arch == 'resnet18': \n",
        "        model = Proteinmodel(models.resnet18)\n",
        "        #model = nn.DataParallel(model)\n",
        "        model = to_device(model, device)\n",
        "    if arch == 'resnet50': model = to_device(Proteinmodel(models.resnet50), device)\n",
        "    model.freeze()    \n",
        "    model, hist = fit_one_cycle(freeze_epochs, max_lr, model, train_dl, val_dl, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)\n",
        "    \n",
        "    history += hist\n",
        "    \n",
        "    model.unfreeze()   \n",
        "    model, hist  = fit_one_cycle(unfreeze_epochs, max_lr/10, model, train_dl, val_dl, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)\n",
        "    \n",
        "    history += hist\n",
        "    print(f'Finishing training of train set for fold=={i}')\n",
        "    print(f'Starting predictions of test set for fold=={i}')\n",
        "    test_preds = predict_dl(test_dl, model)\n",
        "    \n",
        "    predictions.append(test_preds)\n",
        "    print(f'Finish predictions of test set for fold=={i}')\n",
        "    del model\n",
        "    \n",
        "    gc.collect()\n",
        "train_time = (time.time() - since)/60.\n",
        "print(f'Total Training time: {train_time:.2f} minutes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training of train set for fold==0\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.7038, val_loss: 0.6231, val_score: 0.2803\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5836, val_loss: 0.4616, val_score: 0.3723\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3340, val_loss: 0.2855, val_score: 0.6251\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2361, val_loss: 0.2609, val_score: 0.6688\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2223, val_loss: 0.2387, val_score: 0.6939\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2156, val_loss: 0.2591, val_score: 0.6664\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2084, val_loss: 0.2205, val_score: 0.7262\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2059, val_loss: 0.2622, val_score: 0.6675\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2012, val_loss: 0.2003, val_score: 0.7584\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1977, val_loss: 0.2041, val_score: 0.7499\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1936, val_loss: 0.2077, val_score: 0.7405\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1897, val_loss: 0.1915, val_score: 0.7636\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1853, val_loss: 0.1935, val_score: 0.7601\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1807, val_loss: 0.1747, val_score: 0.7884\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1764, val_loss: 0.1756, val_score: 0.7898\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1698, val_loss: 0.1706, val_score: 0.7978\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1635, val_loss: 0.1612, val_score: 0.8095\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1572, val_loss: 0.1577, val_score: 0.8137\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1519, val_loss: 0.1547, val_score: 0.8191\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1471, val_loss: 0.1537, val_score: 0.8190\n",
            "Training complete in 38m 8s\n",
            "Best val Score: 0.819122\n",
            "Best val loss: 0.153713\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1481, val_loss: 0.1541, val_score: 0.8189\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1482, val_loss: 0.1546, val_score: 0.8177\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1489, val_loss: 0.1586, val_score: 0.8156\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1506, val_loss: 0.1574, val_score: 0.8163\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1518, val_loss: 0.1654, val_score: 0.8101\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1515, val_loss: 0.1595, val_score: 0.8141\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1512, val_loss: 0.1626, val_score: 0.8115\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1495, val_loss: 0.1618, val_score: 0.8102\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1490, val_loss: 0.1601, val_score: 0.8119\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1461, val_loss: 0.1573, val_score: 0.8112\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1438, val_loss: 0.1581, val_score: 0.8130\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1409, val_loss: 0.1600, val_score: 0.8115\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1385, val_loss: 0.1546, val_score: 0.8238\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1335, val_loss: 0.1522, val_score: 0.8259\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1306, val_loss: 0.1494, val_score: 0.8281\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1268, val_loss: 0.1511, val_score: 0.8282\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1239, val_loss: 0.1500, val_score: 0.8311\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1208, val_loss: 0.1494, val_score: 0.8297\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1192, val_loss: 0.1491, val_score: 0.8341\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1174, val_loss: 0.1492, val_score: 0.8295\n",
            "Training complete in 38m 8s\n",
            "Best val Score: 0.834069\n",
            "Best val loss: 0.149148\n",
            "Finishing training of train set for fold==0\n",
            "Starting predictions of test set for fold==0\n",
            "Finish predictions of test set for fold==0\n",
            "Starting training of train set for fold==1\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6956, val_loss: 0.6161, val_score: 0.2825\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5790, val_loss: 0.4932, val_score: 0.3616\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3349, val_loss: 0.2905, val_score: 0.6493\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2356, val_loss: 0.2929, val_score: 0.6407\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2217, val_loss: 0.2747, val_score: 0.6725\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2128, val_loss: 0.2877, val_score: 0.6347\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2091, val_loss: 0.2420, val_score: 0.6967\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2039, val_loss: 0.2295, val_score: 0.7099\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.1999, val_loss: 0.2058, val_score: 0.7472\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1987, val_loss: 0.2115, val_score: 0.7385\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1937, val_loss: 0.1899, val_score: 0.7718\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1906, val_loss: 0.1901, val_score: 0.7709\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1865, val_loss: 0.1981, val_score: 0.7663\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1825, val_loss: 0.1829, val_score: 0.7802\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1783, val_loss: 0.1756, val_score: 0.7899\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1697, val_loss: 0.1673, val_score: 0.7979\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1644, val_loss: 0.1637, val_score: 0.8082\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1579, val_loss: 0.1588, val_score: 0.8141\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1523, val_loss: 0.1565, val_score: 0.8154\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1497, val_loss: 0.1558, val_score: 0.8187\n",
            "Training complete in 38m 12s\n",
            "Best val Score: 0.818672\n",
            "Best val loss: 0.155843\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1489, val_loss: 0.1561, val_score: 0.8136\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1487, val_loss: 0.1557, val_score: 0.8141\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1499, val_loss: 0.1589, val_score: 0.8120\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1509, val_loss: 0.1606, val_score: 0.8132\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1527, val_loss: 0.1625, val_score: 0.8003\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1528, val_loss: 0.1595, val_score: 0.8121\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1525, val_loss: 0.1653, val_score: 0.7952\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1502, val_loss: 0.1588, val_score: 0.8096\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1495, val_loss: 0.1591, val_score: 0.8179\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1466, val_loss: 0.1627, val_score: 0.8011\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1448, val_loss: 0.1561, val_score: 0.8165\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1423, val_loss: 0.1553, val_score: 0.8125\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1388, val_loss: 0.1530, val_score: 0.8172\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1350, val_loss: 0.1558, val_score: 0.8154\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1310, val_loss: 0.1532, val_score: 0.8176\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1271, val_loss: 0.1516, val_score: 0.8203\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1236, val_loss: 0.1523, val_score: 0.8192\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1219, val_loss: 0.1512, val_score: 0.8230\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1205, val_loss: 0.1520, val_score: 0.8226\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1184, val_loss: 0.1515, val_score: 0.8229\n",
            "Training complete in 38m 16s\n",
            "Best val Score: 0.823029\n",
            "Best val loss: 0.151177\n",
            "Finishing training of train set for fold==1\n",
            "Starting predictions of test set for fold==1\n",
            "Finish predictions of test set for fold==1\n",
            "Starting training of train set for fold==2\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6940, val_loss: 0.6193, val_score: 0.2814\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5746, val_loss: 0.4510, val_score: 0.4140\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3357, val_loss: 0.2730, val_score: 0.6610\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2360, val_loss: 0.3517, val_score: 0.5669\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2232, val_loss: 0.2426, val_score: 0.6952\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2134, val_loss: 0.2358, val_score: 0.7148\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2100, val_loss: 0.2784, val_score: 0.6502\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2044, val_loss: 0.2217, val_score: 0.7255\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2003, val_loss: 0.2137, val_score: 0.7583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1983, val_loss: 0.2014, val_score: 0.7534\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1946, val_loss: 0.2193, val_score: 0.7183\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1897, val_loss: 0.2001, val_score: 0.7714\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1859, val_loss: 0.2199, val_score: 0.7276\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1816, val_loss: 0.1811, val_score: 0.7874\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1764, val_loss: 0.1809, val_score: 0.7858\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1701, val_loss: 0.1766, val_score: 0.7888\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1646, val_loss: 0.1710, val_score: 0.8035\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1577, val_loss: 0.1645, val_score: 0.8151\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1535, val_loss: 0.1630, val_score: 0.8142\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1491, val_loss: 0.1618, val_score: 0.8158\n",
            "Training complete in 38m 12s\n",
            "Best val Score: 0.815819\n",
            "Best val loss: 0.161820\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1486, val_loss: 0.1617, val_score: 0.8186\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1491, val_loss: 0.1614, val_score: 0.8181\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1500, val_loss: 0.1631, val_score: 0.8184\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1511, val_loss: 0.1650, val_score: 0.8080\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1522, val_loss: 0.1695, val_score: 0.8070\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1525, val_loss: 0.1674, val_score: 0.8119\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1528, val_loss: 0.1656, val_score: 0.8073\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1502, val_loss: 0.1622, val_score: 0.8176\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1478, val_loss: 0.1632, val_score: 0.8143\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1473, val_loss: 0.1622, val_score: 0.8102\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1432, val_loss: 0.1640, val_score: 0.8145\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1424, val_loss: 0.1654, val_score: 0.8114\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1384, val_loss: 0.1645, val_score: 0.8129\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1342, val_loss: 0.1594, val_score: 0.8171\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1319, val_loss: 0.1592, val_score: 0.8162\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1288, val_loss: 0.1584, val_score: 0.8253\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1243, val_loss: 0.1563, val_score: 0.8236\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1223, val_loss: 0.1548, val_score: 0.8272\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1209, val_loss: 0.1547, val_score: 0.8241\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1196, val_loss: 0.1552, val_score: 0.8243\n",
            "Training complete in 38m 4s\n",
            "Best val Score: 0.827247\n",
            "Best val loss: 0.154689\n",
            "Finishing training of train set for fold==2\n",
            "Starting predictions of test set for fold==2\n",
            "Finish predictions of test set for fold==2\n",
            "Starting training of train set for fold==3\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6946, val_loss: 0.6159, val_score: 0.2814\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5747, val_loss: 0.4571, val_score: 0.4040\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3336, val_loss: 0.2881, val_score: 0.6492\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2363, val_loss: 0.2619, val_score: 0.6706\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2209, val_loss: 0.2333, val_score: 0.6977\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2137, val_loss: 0.3240, val_score: 0.6148\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2085, val_loss: 0.2814, val_score: 0.6612\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2033, val_loss: 0.2226, val_score: 0.7145\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2010, val_loss: 0.2109, val_score: 0.7352\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1973, val_loss: 0.2286, val_score: 0.7166\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1934, val_loss: 0.2085, val_score: 0.7529\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1911, val_loss: 0.1966, val_score: 0.7590\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1858, val_loss: 0.1947, val_score: 0.7692\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1815, val_loss: 0.1786, val_score: 0.7921\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1764, val_loss: 0.1748, val_score: 0.7933\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1696, val_loss: 0.1728, val_score: 0.7908\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1636, val_loss: 0.1634, val_score: 0.8091\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1574, val_loss: 0.1601, val_score: 0.8154\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1513, val_loss: 0.1565, val_score: 0.8136\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1483, val_loss: 0.1564, val_score: 0.8163\n",
            "Training complete in 38m 14s\n",
            "Best val Score: 0.816338\n",
            "Best val loss: 0.156443\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1475, val_loss: 0.1560, val_score: 0.8160\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1481, val_loss: 0.1577, val_score: 0.8152\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1490, val_loss: 0.1576, val_score: 0.8115\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1506, val_loss: 0.1612, val_score: 0.8085\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1515, val_loss: 0.1608, val_score: 0.8090\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1512, val_loss: 0.1626, val_score: 0.8064\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1511, val_loss: 0.1657, val_score: 0.8027\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1492, val_loss: 0.1659, val_score: 0.8041\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1486, val_loss: 0.1642, val_score: 0.8107\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1462, val_loss: 0.1577, val_score: 0.8101\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1423, val_loss: 0.1634, val_score: 0.8041\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1405, val_loss: 0.1566, val_score: 0.8166\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1365, val_loss: 0.1596, val_score: 0.8159\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1341, val_loss: 0.1573, val_score: 0.8161\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1299, val_loss: 0.1545, val_score: 0.8229\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1253, val_loss: 0.1535, val_score: 0.8192\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1216, val_loss: 0.1534, val_score: 0.8191\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1193, val_loss: 0.1528, val_score: 0.8225\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1172, val_loss: 0.1531, val_score: 0.8229\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1167, val_loss: 0.1533, val_score: 0.8236\n",
            "Training complete in 38m 9s\n",
            "Best val Score: 0.823563\n",
            "Best val loss: 0.152769\n",
            "Finishing training of train set for fold==3\n",
            "Starting predictions of test set for fold==3\n",
            "Finish predictions of test set for fold==3\n",
            "Starting training of train set for fold==4\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6960, val_loss: 0.6099, val_score: 0.2778\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5799, val_loss: 0.5054, val_score: 0.4202\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3372, val_loss: 0.2451, val_score: 0.6963\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2365, val_loss: 0.2471, val_score: 0.6801\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2216, val_loss: 0.2116, val_score: 0.7434\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2171, val_loss: 0.2315, val_score: 0.7058\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2092, val_loss: 0.2230, val_score: 0.7078\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2042, val_loss: 0.2028, val_score: 0.7505\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2017, val_loss: 0.3065, val_score: 0.5896\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1990, val_loss: 0.2013, val_score: 0.7441\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1951, val_loss: 0.2119, val_score: 0.7356\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1915, val_loss: 0.2042, val_score: 0.7454\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1884, val_loss: 0.1900, val_score: 0.7683\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1826, val_loss: 0.1692, val_score: 0.8025\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1769, val_loss: 0.1653, val_score: 0.8062\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1700, val_loss: 0.1629, val_score: 0.8077\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1640, val_loss: 0.1611, val_score: 0.8085\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1576, val_loss: 0.1519, val_score: 0.8233\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1525, val_loss: 0.1476, val_score: 0.8322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1499, val_loss: 0.1474, val_score: 0.8313\n",
            "Training complete in 38m 13s\n",
            "Best val Score: 0.832171\n",
            "Best val loss: 0.147446\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1491, val_loss: 0.1474, val_score: 0.8295\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1487, val_loss: 0.1476, val_score: 0.8332\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1496, val_loss: 0.1486, val_score: 0.8302\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1522, val_loss: 0.1536, val_score: 0.8189\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1521, val_loss: 0.1543, val_score: 0.8219\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1535, val_loss: 0.1519, val_score: 0.8221\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1522, val_loss: 0.1568, val_score: 0.8157\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1506, val_loss: 0.1534, val_score: 0.8227\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1490, val_loss: 0.1489, val_score: 0.8295\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1458, val_loss: 0.1576, val_score: 0.8240\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1440, val_loss: 0.1521, val_score: 0.8264\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1421, val_loss: 0.1481, val_score: 0.8329\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1375, val_loss: 0.1470, val_score: 0.8258\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1349, val_loss: 0.1462, val_score: 0.8329\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1314, val_loss: 0.1455, val_score: 0.8363\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1277, val_loss: 0.1459, val_score: 0.8344\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1256, val_loss: 0.1442, val_score: 0.8372\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1215, val_loss: 0.1442, val_score: 0.8365\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1200, val_loss: 0.1451, val_score: 0.8402\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1189, val_loss: 0.1440, val_score: 0.8385\n",
            "Training complete in 38m 9s\n",
            "Best val Score: 0.840161\n",
            "Best val loss: 0.143967\n",
            "Finishing training of train set for fold==4\n",
            "Starting predictions of test set for fold==4\n",
            "Finish predictions of test set for fold==4\n",
            "Starting training of train set for fold==5\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6902, val_loss: 0.6091, val_score: 0.2828\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5686, val_loss: 1.1398, val_score: 0.4269\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3324, val_loss: 0.2917, val_score: 0.6756\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2339, val_loss: 0.2281, val_score: 0.7248\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2205, val_loss: 0.2801, val_score: 0.6453\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2142, val_loss: 0.2204, val_score: 0.7279\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2069, val_loss: 0.2149, val_score: 0.7545\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2041, val_loss: 0.2086, val_score: 0.7455\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2000, val_loss: 0.2192, val_score: 0.7236\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1975, val_loss: 0.2331, val_score: 0.7067\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1951, val_loss: 0.2012, val_score: 0.7558\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1912, val_loss: 0.2072, val_score: 0.7457\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1863, val_loss: 0.1832, val_score: 0.7836\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1814, val_loss: 0.1789, val_score: 0.7905\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1767, val_loss: 0.1777, val_score: 0.7981\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1708, val_loss: 0.1675, val_score: 0.8083\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1644, val_loss: 0.1631, val_score: 0.8158\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1580, val_loss: 0.1568, val_score: 0.8169\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1521, val_loss: 0.1529, val_score: 0.8270\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1492, val_loss: 0.1526, val_score: 0.8273\n",
            "Training complete in 38m 8s\n",
            "Best val Score: 0.827296\n",
            "Best val loss: 0.152606\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1481, val_loss: 0.1524, val_score: 0.8260\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1481, val_loss: 0.1536, val_score: 0.8273\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1497, val_loss: 0.1596, val_score: 0.8205\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1513, val_loss: 0.1577, val_score: 0.8187\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1522, val_loss: 0.1583, val_score: 0.8209\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1521, val_loss: 0.1596, val_score: 0.8155\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1514, val_loss: 0.1665, val_score: 0.8068\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1501, val_loss: 0.1601, val_score: 0.8202\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1492, val_loss: 0.1546, val_score: 0.8256\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1473, val_loss: 0.1565, val_score: 0.8200\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1433, val_loss: 0.1528, val_score: 0.8280\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1408, val_loss: 0.1548, val_score: 0.8243\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1389, val_loss: 0.1563, val_score: 0.8220\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1350, val_loss: 0.1527, val_score: 0.8265\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1306, val_loss: 0.1502, val_score: 0.8266\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1268, val_loss: 0.1503, val_score: 0.8279\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1244, val_loss: 0.1492, val_score: 0.8323\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1221, val_loss: 0.1498, val_score: 0.8364\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1196, val_loss: 0.1494, val_score: 0.8354\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1194, val_loss: 0.1496, val_score: 0.8348\n",
            "Training complete in 38m 10s\n",
            "Best val Score: 0.836356\n",
            "Best val loss: 0.149205\n",
            "Finishing training of train set for fold==5\n",
            "Starting predictions of test set for fold==5\n",
            "Finish predictions of test set for fold==5\n",
            "Starting training of train set for fold==6\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.7022, val_loss: 0.6138, val_score: 0.2822\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5823, val_loss: 0.4404, val_score: 0.4053\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3327, val_loss: 0.2346, val_score: 0.7175\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2354, val_loss: 0.2188, val_score: 0.7318\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2212, val_loss: 0.2371, val_score: 0.7146\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2148, val_loss: 0.2094, val_score: 0.7485\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2093, val_loss: 0.3249, val_score: 0.5673\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2052, val_loss: 0.2583, val_score: 0.6777\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2011, val_loss: 0.2302, val_score: 0.7201\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1978, val_loss: 0.1929, val_score: 0.7680\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1941, val_loss: 0.2209, val_score: 0.7276\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1917, val_loss: 0.1868, val_score: 0.7789\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1864, val_loss: 0.1985, val_score: 0.7638\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1822, val_loss: 0.1783, val_score: 0.7909\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1771, val_loss: 0.1713, val_score: 0.7968\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1716, val_loss: 0.1717, val_score: 0.7941\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1652, val_loss: 0.1564, val_score: 0.8198\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1580, val_loss: 0.1542, val_score: 0.8220\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1523, val_loss: 0.1504, val_score: 0.8313\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1491, val_loss: 0.1499, val_score: 0.8294\n",
            "Training complete in 38m 3s\n",
            "Best val Score: 0.831261\n",
            "Best val loss: 0.149865\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1483, val_loss: 0.1498, val_score: 0.8293\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1483, val_loss: 0.1507, val_score: 0.8267\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1493, val_loss: 0.1525, val_score: 0.8293\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1512, val_loss: 0.1571, val_score: 0.8143\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1536, val_loss: 0.1634, val_score: 0.8051\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1530, val_loss: 0.1589, val_score: 0.8123\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1514, val_loss: 0.1589, val_score: 0.8196\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1511, val_loss: 0.1597, val_score: 0.8181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1496, val_loss: 0.1533, val_score: 0.8194\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1464, val_loss: 0.1546, val_score: 0.8227\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1437, val_loss: 0.1543, val_score: 0.8191\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1417, val_loss: 0.1504, val_score: 0.8294\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1370, val_loss: 0.1506, val_score: 0.8292\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1348, val_loss: 0.1510, val_score: 0.8298\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1301, val_loss: 0.1476, val_score: 0.8323\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1268, val_loss: 0.1463, val_score: 0.8336\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1232, val_loss: 0.1471, val_score: 0.8366\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1213, val_loss: 0.1459, val_score: 0.8357\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1200, val_loss: 0.1465, val_score: 0.8341\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1190, val_loss: 0.1469, val_score: 0.8347\n",
            "Training complete in 38m 1s\n",
            "Best val Score: 0.836579\n",
            "Best val loss: 0.145946\n",
            "Finishing training of train set for fold==6\n",
            "Starting predictions of test set for fold==6\n",
            "Finish predictions of test set for fold==6\n",
            "Starting training of train set for fold==7\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6949, val_loss: 0.6090, val_score: 0.2817\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5754, val_loss: 0.4374, val_score: 0.4254\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3319, val_loss: 0.2394, val_score: 0.7105\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2336, val_loss: 0.2663, val_score: 0.6699\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2199, val_loss: 0.2270, val_score: 0.7181\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2154, val_loss: 0.3213, val_score: 0.6324\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2080, val_loss: 0.2599, val_score: 0.6842\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2031, val_loss: 0.2438, val_score: 0.6865\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.1995, val_loss: 0.2034, val_score: 0.7505\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1967, val_loss: 0.2052, val_score: 0.7470\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1936, val_loss: 0.1948, val_score: 0.7707\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1910, val_loss: 0.2011, val_score: 0.7607\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1868, val_loss: 0.1861, val_score: 0.7787\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1811, val_loss: 0.1934, val_score: 0.7657\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1750, val_loss: 0.1774, val_score: 0.7947\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1698, val_loss: 0.1773, val_score: 0.7904\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1644, val_loss: 0.1612, val_score: 0.8059\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1568, val_loss: 0.1588, val_score: 0.8160\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1518, val_loss: 0.1556, val_score: 0.8173\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1481, val_loss: 0.1556, val_score: 0.8145\n",
            "Training complete in 38m 13s\n",
            "Best val Score: 0.817269\n",
            "Best val loss: 0.155625\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1480, val_loss: 0.1557, val_score: 0.8174\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1485, val_loss: 0.1561, val_score: 0.8167\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1489, val_loss: 0.1584, val_score: 0.8045\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1494, val_loss: 0.1584, val_score: 0.8121\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1517, val_loss: 0.1620, val_score: 0.8096\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1529, val_loss: 0.1647, val_score: 0.8067\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1523, val_loss: 0.1639, val_score: 0.8092\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1509, val_loss: 0.1593, val_score: 0.8154\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1486, val_loss: 0.1580, val_score: 0.8175\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1448, val_loss: 0.1614, val_score: 0.8160\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1425, val_loss: 0.1590, val_score: 0.8174\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1406, val_loss: 0.1573, val_score: 0.8177\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1373, val_loss: 0.1531, val_score: 0.8260\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1347, val_loss: 0.1559, val_score: 0.8211\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1299, val_loss: 0.1530, val_score: 0.8251\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1256, val_loss: 0.1528, val_score: 0.8259\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1225, val_loss: 0.1523, val_score: 0.8287\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1209, val_loss: 0.1513, val_score: 0.8312\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1193, val_loss: 0.1520, val_score: 0.8274\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1177, val_loss: 0.1512, val_score: 0.8306\n",
            "Training complete in 38m 5s\n",
            "Best val Score: 0.831182\n",
            "Best val loss: 0.151176\n",
            "Finishing training of train set for fold==7\n",
            "Starting predictions of test set for fold==7\n",
            "Finish predictions of test set for fold==7\n",
            "Starting training of train set for fold==8\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6910, val_loss: 0.5914, val_score: 0.2832\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5711, val_loss: 0.5018, val_score: 0.4403\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3352, val_loss: 0.2596, val_score: 0.6987\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2365, val_loss: 0.2492, val_score: 0.6814\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2225, val_loss: 0.2120, val_score: 0.7460\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2144, val_loss: 0.2498, val_score: 0.6836\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2105, val_loss: 0.2428, val_score: 0.6975\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2047, val_loss: 0.2187, val_score: 0.7186\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2015, val_loss: 0.2129, val_score: 0.7369\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1974, val_loss: 0.2099, val_score: 0.7356\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1948, val_loss: 0.1899, val_score: 0.7658\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1904, val_loss: 0.1850, val_score: 0.7723\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1869, val_loss: 0.1740, val_score: 0.7900\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1822, val_loss: 0.1733, val_score: 0.7964\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1775, val_loss: 0.1697, val_score: 0.8023\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1726, val_loss: 0.1668, val_score: 0.7980\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1652, val_loss: 0.1587, val_score: 0.8122\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1582, val_loss: 0.1539, val_score: 0.8231\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1540, val_loss: 0.1511, val_score: 0.8183\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1496, val_loss: 0.1508, val_score: 0.8227\n",
            "Training complete in 38m 14s\n",
            "Best val Score: 0.823067\n",
            "Best val loss: 0.150831\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1494, val_loss: 0.1507, val_score: 0.8234\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1487, val_loss: 0.1512, val_score: 0.8209\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1498, val_loss: 0.1543, val_score: 0.8175\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1515, val_loss: 0.1567, val_score: 0.8119\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1535, val_loss: 0.1550, val_score: 0.8185\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1530, val_loss: 0.1604, val_score: 0.8099\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1526, val_loss: 0.1602, val_score: 0.8143\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1501, val_loss: 0.1548, val_score: 0.8233\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1486, val_loss: 0.1568, val_score: 0.8108\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1472, val_loss: 0.1560, val_score: 0.8122\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1449, val_loss: 0.1537, val_score: 0.8193\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1421, val_loss: 0.1486, val_score: 0.8280\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1379, val_loss: 0.1497, val_score: 0.8267\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1347, val_loss: 0.1497, val_score: 0.8249\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1305, val_loss: 0.1464, val_score: 0.8301\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1274, val_loss: 0.1470, val_score: 0.8289\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1239, val_loss: 0.1460, val_score: 0.8283\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1211, val_loss: 0.1457, val_score: 0.8305\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1187, val_loss: 0.1467, val_score: 0.8296\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1186, val_loss: 0.1458, val_score: 0.8313\n",
            "Training complete in 38m 7s\n",
            "Best val Score: 0.831337\n",
            "Best val loss: 0.145701\n",
            "Finishing training of train set for fold==8\n",
            "Starting predictions of test set for fold==8\n",
            "Finish predictions of test set for fold==8\n",
            "Starting training of train set for fold==9\n",
            "Epoch [0], last_lr: 0.0001, train_loss: 0.6970, val_loss: 0.6090, val_score: 0.2852\n",
            "Epoch [1], last_lr: 0.0003, train_loss: 0.5747, val_loss: 0.5794, val_score: 0.4566\n",
            "Epoch [2], last_lr: 0.0005, train_loss: 0.3311, val_loss: 0.2714, val_score: 0.6587\n",
            "Epoch [3], last_lr: 0.0008, train_loss: 0.2368, val_loss: 0.2954, val_score: 0.6425\n",
            "Epoch [4], last_lr: 0.0009, train_loss: 0.2220, val_loss: 0.2479, val_score: 0.6625\n",
            "Epoch [5], last_lr: 0.0010, train_loss: 0.2138, val_loss: 0.2311, val_score: 0.7126\n",
            "Epoch [6], last_lr: 0.0010, train_loss: 0.2100, val_loss: 0.2238, val_score: 0.7285\n",
            "Epoch [7], last_lr: 0.0010, train_loss: 0.2044, val_loss: 0.2269, val_score: 0.7261\n",
            "Epoch [8], last_lr: 0.0009, train_loss: 0.2004, val_loss: 0.2905, val_score: 0.6580\n",
            "Epoch [9], last_lr: 0.0008, train_loss: 0.1977, val_loss: 0.2220, val_score: 0.7290\n",
            "Epoch [10], last_lr: 0.0007, train_loss: 0.1942, val_loss: 0.2047, val_score: 0.7559\n",
            "Epoch [11], last_lr: 0.0006, train_loss: 0.1883, val_loss: 0.2094, val_score: 0.7522\n",
            "Epoch [12], last_lr: 0.0005, train_loss: 0.1861, val_loss: 0.1941, val_score: 0.7663\n",
            "Epoch [13], last_lr: 0.0004, train_loss: 0.1812, val_loss: 0.1952, val_score: 0.7630\n",
            "Epoch [14], last_lr: 0.0003, train_loss: 0.1773, val_loss: 0.1814, val_score: 0.7856\n",
            "Epoch [15], last_lr: 0.0002, train_loss: 0.1702, val_loss: 0.1703, val_score: 0.8052\n",
            "Epoch [16], last_lr: 0.0001, train_loss: 0.1638, val_loss: 0.1650, val_score: 0.8110\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1567, val_loss: 0.1631, val_score: 0.8114\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1515, val_loss: 0.1605, val_score: 0.8218\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1489, val_loss: 0.1602, val_score: 0.8213\n",
            "Training complete in 38m 11s\n",
            "Best val Score: 0.821828\n",
            "Best val loss: 0.160231\n",
            "Epoch [0], last_lr: 0.0000, train_loss: 0.1480, val_loss: 0.1597, val_score: 0.8197\n",
            "Epoch [1], last_lr: 0.0000, train_loss: 0.1485, val_loss: 0.1613, val_score: 0.8171\n",
            "Epoch [2], last_lr: 0.0001, train_loss: 0.1503, val_loss: 0.1624, val_score: 0.8183\n",
            "Epoch [3], last_lr: 0.0001, train_loss: 0.1509, val_loss: 0.1637, val_score: 0.8139\n",
            "Epoch [4], last_lr: 0.0001, train_loss: 0.1523, val_loss: 0.1642, val_score: 0.8155\n",
            "Epoch [5], last_lr: 0.0001, train_loss: 0.1517, val_loss: 0.1680, val_score: 0.8058\n",
            "Epoch [6], last_lr: 0.0001, train_loss: 0.1510, val_loss: 0.1679, val_score: 0.8097\n",
            "Epoch [7], last_lr: 0.0001, train_loss: 0.1504, val_loss: 0.1671, val_score: 0.8083\n",
            "Epoch [8], last_lr: 0.0001, train_loss: 0.1483, val_loss: 0.1675, val_score: 0.8150\n",
            "Epoch [9], last_lr: 0.0001, train_loss: 0.1468, val_loss: 0.1629, val_score: 0.8126\n",
            "Epoch [10], last_lr: 0.0001, train_loss: 0.1435, val_loss: 0.1618, val_score: 0.8174\n",
            "Epoch [11], last_lr: 0.0001, train_loss: 0.1418, val_loss: 0.1634, val_score: 0.8179\n",
            "Epoch [12], last_lr: 0.0001, train_loss: 0.1373, val_loss: 0.1615, val_score: 0.8172\n",
            "Epoch [13], last_lr: 0.0000, train_loss: 0.1344, val_loss: 0.1573, val_score: 0.8236\n",
            "Epoch [14], last_lr: 0.0000, train_loss: 0.1309, val_loss: 0.1587, val_score: 0.8192\n",
            "Epoch [15], last_lr: 0.0000, train_loss: 0.1275, val_loss: 0.1567, val_score: 0.8295\n",
            "Epoch [16], last_lr: 0.0000, train_loss: 0.1251, val_loss: 0.1567, val_score: 0.8255\n",
            "Epoch [17], last_lr: 0.0000, train_loss: 0.1215, val_loss: 0.1574, val_score: 0.8288\n",
            "Epoch [18], last_lr: 0.0000, train_loss: 0.1212, val_loss: 0.1565, val_score: 0.8275\n",
            "Epoch [19], last_lr: 0.0000, train_loss: 0.1194, val_loss: 0.1570, val_score: 0.8276\n",
            "Training complete in 38m 10s\n",
            "Best val Score: 0.829456\n",
            "Best val loss: 0.156520\n",
            "Finishing training of train set for fold==9\n",
            "Starting predictions of test set for fold==9\n",
            "Finish predictions of test set for fold==9\n",
            "Total Training time: 766.26 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-ZktAkH9Lqn",
        "colab_type": "text"
      },
      "source": [
        "## Ensemble the models by averaging the predictions from each fold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDlKE-sD9Lqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction_cv = torch.stack(predictions).mean(axis=0)\n",
        "decoded_predictions = prediction_cv > threshold\n",
        "submission[\"Label\"] = [decode_target(t.tolist()) for t in  decoded_predictions]\n",
        "submission.to_csv(f\"submission-mean-cv-{threshold}.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcEYl12L9Lqq",
        "colab_type": "code",
        "colab": {},
        "outputId": "d325c960-493a-4e66-ab34-d518595a1864"
      },
      "source": [
        "print(len(prediction_cv))\n",
        "print(len(test_preds))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8243\n",
            "8243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc22dh6R9Lqy",
        "colab_type": "code",
        "colab": {},
        "outputId": "1e90a204-9215-45a2-a797-44a67e529c7c"
      },
      "source": [
        "submission.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24117</td>\n",
              "      <td>4 8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15322</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14546</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8079</td>\n",
              "      <td>0 6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13192</td>\n",
              "      <td>3 4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>25927</td>\n",
              "      <td>1 4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3372</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>21781</td>\n",
              "      <td>3 6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2847</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>16413</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Image Label\n",
              "0  24117   4 8\n",
              "1  15322     4\n",
              "2  14546     6\n",
              "3   8079   0 6\n",
              "4  13192   3 4\n",
              "5  25927   1 4\n",
              "6   3372     0\n",
              "7  21781   3 6\n",
              "8   2847     4\n",
              "9  16413     9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQHpi54d9Lq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arch = 'resnet18'\n",
        "epochs = 20\n",
        "size = 512\n",
        "if size == 256: batch_size = 128\n",
        "if size == 512: batch_size = 64\n",
        "    \n",
        "nfolds = 10\n",
        "threshold = 0.3\n",
        "SEED = 2020\n",
        "max_lr = 0.001\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "opt_func = torch.optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCpVvY4D9Lq5",
        "colab_type": "code",
        "colab": {},
        "outputId": "e241cc4d-74f7-41ea-dc8f-a0eb7f720696"
      },
      "source": [
        "import jovian\n",
        "jovian.reset()\n",
        "jovian.log_hyperparams(arch=arch, \n",
        "                       epochs=epochs, \n",
        "                       lr=max_lr, \n",
        "                       scheduler='one-cycle', \n",
        "                       weight_decay=weight_decay, \n",
        "                       grad_clip=grad_clip,\n",
        "                       opt=opt_func.__name__,\n",
        "                       threshold=threshold,\n",
        "                       activation='sigmoid',\n",
        "                       nfolds=nfolds,\n",
        "                       batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Hyperparams logged.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMTRXr4Z9Lq9",
        "colab_type": "code",
        "colab": {},
        "outputId": "8c73dbed-2bf8-4b29-9989-54b6b9bcfb29"
      },
      "source": [
        "jovian.log_metrics(val_loss=history[-1]['val_loss'], \n",
        "                   val_score=history[-1]['val_score'],\n",
        "                   train_loss=history[-1]['train_loss'],\n",
        "                   time=(time.time() - since)/60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[jovian] Metrics logged.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB2gBwWL9LrC",
        "colab_type": "text"
      },
      "source": [
        "I've trained for a few epochs with image_size=256 just for the sharing purposes. Training for longer times with bigger images and tweaked hyperparameters can make your model a lot better."
      ]
    }
  ]
}